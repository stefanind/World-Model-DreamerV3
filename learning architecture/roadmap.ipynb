{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3361d24",
   "metadata": {},
   "source": [
    "# Roadmap of DreamerV3 main code loop\n",
    "\n",
    "Goal: want to know how data flows from beginning to end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8712f37",
   "metadata": {},
   "source": [
    "- load config file\n",
    "    - likely a yaml with attridict so I can easily call from it as if they are class attributes\n",
    "- set seeds\n",
    "    - if using many different libraries with different seeds, ensure to call seeds from each of them\n",
    "    - e.g., numpy -> np.random.seed, torch -> torch.manual_seed, etc...\n",
    "- set up folders for evaluations, checkpoints, plots, metrics, etc.\n",
    "- initialize the agent environment, likely using gymnasium, and get the env properties\n",
    "    - check formal definition of the space to ensure proper\n",
    "    - e.g., check if wanting continuous or discrete (gym.spaces.Box vs gym.spaces.Discrete)\n",
    "    - NOTE: will need extensive review when deciding what data to work with to ensure it is handled properly and that the encoder/decoder works with it\n",
    "    - the original DreamerV3 (danijar) has extensive logic for handling all kinds of inputs (his encoder/decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bfb4d353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-1.  0.  0.], 1.0, (3,), float32)\n",
      "Box(0, 255, (96, 96, 3), uint8)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CarRacing-v3\")\n",
    "print(env.action_space)\n",
    "# Box (-2.0, 2.0, (1,), float32)\n",
    "# low = -2.0\n",
    "# high = 2.0\n",
    "# shape = (1,)\n",
    "print(env.observation_space)\n",
    "# Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
    "# [-1. -1. -8.]\n",
    "# [1. 1. 8.]\n",
    "# these spaces are the formal definitions (ranges) of allowed values for every element in that space\n",
    "# e.g.: \n",
    "# observation_space = obs\n",
    "# obs[0] ∈ [−1, +1]\n",
    "# obs[1] ∈ [−1, +1]\n",
    "# obs[2] ∈ [−8, +8]\n",
    "# dtype(obs) = float32\n",
    "# size cannot be greater than "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b86eb42",
   "metadata": {},
   "source": [
    "- initialize the core dreamer agent using the env properties and config (decision on architecture)\n",
    "    - NOTE: core architecture not gone through here\n",
    "- initialize dreamer interaction with the environment for data collection\n",
    "    - start the loop for how many episodes (how many times we get 'done') of data wanted, and keep track of the scores outside of it\n",
    "        - init the states (action, latent, and hidden) with zeros and a batch of 1\n",
    "        - reset the environment to produce a new observation (.reset() will trigger .observation() automatically in gymnasium)\n",
    "        - encode the new observation to get the initial starting point\n",
    "        - start the loop with the break condition when 'done' appears\n",
    "            - call GRU to get hidden state\n",
    "            - using hidden state + obs, get the stochastic latent from posterior network\n",
    "            - get the action from the actor using the hidden and latent states\n",
    "            - using the action, take the next step (.step() in the env)\n",
    "            - append to the buffer to collect the data\n",
    "                - want the current observation, the action taken, the reward, the next observation, and whether it is 'done' or not\n",
    "            - set the observation to the new one gotten from the action that the actor decided to take (observation = next_observation)\n",
    "            - keep track of metrics, e.g., the score based on rewards, step count taken, total episodes run through, total env steps taken\n",
    "        - returns the avg score based on the number of episodes run through"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c233f20",
   "metadata": {},
   "source": [
    "### Now that we have the first burst of data and model initialized, training begins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f53989d",
   "metadata": {},
   "source": [
    "- determine how many steps wanted and how many replays wanted\n",
    "    - i.e., how many total gradient steps (gradient updates) and how many runs through the current data before adding more to the buffer\n",
    "- initialize outer loop: the number of total iterations (gradient steps // replays); \n",
    "    - initialize inner loop: the amount of replays (how many times sampled from the current buffer before adding more)\n",
    "        - sample from the buffer of batch size and sequence length (how many actions taken per sample)\n",
    "        - get the states from the world model training\n",
    "            - encodes observations from the observation data\n",
    "            - loop over the length of sequence length (time interval)\n",
    "                - get current recurrent state using prev action sequence (t-1), previous latent state, and previous recurrent state\n",
    "                - get the logits from the prior network (no observation needed as input for this network)\n",
    "                - get the logits and the latent stochastic from the posterior network (requires current recurrent state and the current encoded observation)\n",
    "                - append all this acquired data to their respected lists\n",
    "                - set the new calculated states to their previous variables (e.g., prevRecurrent = recurrent)\n",
    "            - calculate the loss for decoding\n",
    "                - using the hidden states and latent states, decode to make observations\n",
    "                - model these decoded observations as a normal distribution \n",
    "                - take the log probs of this distribution and see where the true observation are\n",
    "                    - if true obs close to the predicted distribution mean, then loss is low\n",
    "                - compute the mean of all loss calculations, gotten from each sample in the batch, to get one scalar value representing overall loss\n",
    "            - calculate the loss for the reward (a scalar value on how good or bad the action was)\n",
    "                - feed the hidden + stochastic states through the reward network\n",
    "                - get out a normal distribution, specifiaclly, the mean and std\n",
    "                - compute the mean of all loss calculations\n",
    "            - calculate the loss for the prior and posterior networks\n",
    "                - require computing KL divergence between both networks\n",
    "                    - ensure that gradients flow properly through the prior and posterior (compute two kl for posterior and prior)\n",
    "                    - clamp the bottom end to 'freeNats' to make sure that KL doesn't go down to 0 (posterior collapse)\n",
    "                    - make sure that the prior network follows the posterior \n",
    "                - get the total kl loss (mean of post + prior net loss)\n",
    "            - using all losses (decoder, reward and kl), get world model loss (sum of all losses)\n",
    "                - if continuation is on (i.e., if we want the continuepredictor to try to figure out when the action sequence terminates--'done')\n",
    "                    - compute the continuationLoss and add this to the world model loss\n",
    "            - zero grad, backward pass, parameter clipping, optimizer step\n",
    "            - report metrics (worldmodel loss, decoder loss, reward loss, kl loss)\n",
    "            - return the metrics and the hidden + stochastic state\n",
    "        - get the behavior metrics from behavior training\n",
    "            - \n",
    "\n",
    "        - save checkpoints, metrics and/or evaluate if wanted\n",
    "    - add more data to the buffer before running the next inner loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc17bfed",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
