{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c832415",
   "metadata": {},
   "source": [
    "### gathering data\n",
    "- Start with a state (observation)--the one the env gives (observation_t)\n",
    "- agent chooses an action based on the observation\n",
    "- env executes the action and updates the internal state\n",
    "- env returns results:\n",
    "    - nextobs: the new state (obs) after the action\n",
    "    - reward: the scalar value for that action\n",
    "    - terminated: whether episode ended naturally\n",
    "    - truncated: whether episode ended for some gamestopping reason (e.g., time limit)\n",
    "    - done = terminated or truncated\n",
    "\n",
    "### sampling data\n",
    "- pull out a batch of samples with each being of size sequence length\n",
    "- it is a dict with 5 key-val pairs\n",
    "    - observation, action, reward, nextObs, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7539211f",
   "metadata": {},
   "source": [
    "### main training loop  \n",
    "- outer loop\n",
    "    - how many environment gathering sessions are we going to do, i.e., run an environment to load the buffer.\n",
    "- inner loop\n",
    "    - how many iterations of sampling will we take from a particular environment gathering session, i.e., how many times will we sample from a loaded buffer.\n",
    "    - each time we train the world model with a sample, we then use the hidden and stochastic state to run behavior training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c97cc7b",
   "metadata": {},
   "source": [
    "### the world model loop  \n",
    "- we start with the t-1 action for the initial recurrent state (loop begins at t=1)\n",
    "    - input prev recurrent state, prev latent state, and prev action (t-1 action in the data sequence)\n",
    "    - outputs the recurrent state (h_t)\n",
    "    - use this to input into prior net and posterior net\n",
    "    - important to know that this recurrent state is used with the t encoded observation, so we essentially don't use the 0th encoded observation.\n",
    "    - this means that we're only using the hidden state that has the previous action that led to the current observation to train the posterior net.\n",
    "    - this also means we get 1 less in the batch length because actions are transitions from observation to observation, i.e., the transition from each t to the next one is gotten from an action (t -> t+1 -> ... -> t + batchlength-1) so there will be one less action than observations in the sequence. Hence, because we begin using observations from t, we are reducing the amount of the batch length by 1.\n",
    "- after stacking each hidden states output from each t back together to get that batch length - 1, we use the concatenated hidden states with the posteriors stochastic, giving us (batchSize, batchLength-1, recurrentSize + latentLength*latentClasses). This is what is outputted by the world model to be used by the imagination rollout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889758fd",
   "metadata": {},
   "source": [
    "### behavior training loop (imagination rollout)  \n",
    "- from the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5e4c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
